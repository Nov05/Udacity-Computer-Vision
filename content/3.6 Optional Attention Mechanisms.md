

## Computer Vision - 3.6 Attention Mechanism

1. Introduction to Attention   
https://youtu.be/NCn97L5WbCY

2. Enconders and Decoders    
https://youtu.be/tDJBDwriJYQ    
https://youtu.be/dkHdEAJnV_w   

3. Elective: Text Sentiment Analysis    
If you would like more practice with analyzing sequences of words with a simple network, now would be a great time to check out the elective section: Text Sentiment Analysis. In this section, Andrew Trask teaches you how to convert words into vectors and then analyze the sentiment of these vectors. He goes through constructing and tuning a model and addresses some common errors in text analysis. This section does not contain material that is required to complete this program or the project in this section, but it is interesting and you may find it useful!   

4. Sequence to Sequence Recap   
https://youtu.be/MRPHIPR0pGE   

5. Encoding - Attention Overview   
https://youtu.be/IctAnMaVUKc   

6. Decoding - Attention Overview   
https://youtu.be/DJxiPd585GY   

7. Quiz: Attention Overview    
QUESTION 1 OF 3    
True or False: A sequence-to-sequence model processes the input sequence all in one step 【False】
```
True - a seq2seq model processes its inputs by looking at the entire input sequence all at once  
False - a seq2seq model works by feeding one element of the input sequence at a time to the encoder
```
QUESTION 2 OF 3  
Which of the following is a limitation of seq2seq models which can be solved using attention methods? 【b and c】
```
a) Inability to use word embeddings   
b) The fixed size of the context matrix passed from the encoder to the decoder is a bottleneck
c) Difficulty of encoding long sequences and recalling long-term dependancies
```
QUESTION 3 OF 3  
How large is the context matrix in an attention seq2seq model? 【b】
```
a) A fixed size - a single vector
b) Depends on the length of the input sequence
```

8. Attention Encoder    
https://youtu.be/sphe9LDT4rA   

9. Attention Decoder   
https://youtu.be/5mMz6nN9_Ss   

10. Quiz: Attention Encoder & Decoder    
QUESTION 1 OF 3    
In machine translation applications, the encoder and decoder are typically 【b】
```
a) Generative Adversarial Networks (GANs)
b) Recurrent Neural Networks (Typically vanilla RNN, LSTM, or GRU)
c) Mentats
```
QUESTION 2 OF 3   
What's a more reasonable embedding size for a real-world application? 【200】
```
4, 200, 6000
```
QUESTION 3 OF 3   
What are the steps that require calculating an attention vector in a seq2seq model with attention? 【c】  
```
a) Every time step in the model (both encoder and decoder)
b) Every time step in the encoder only
c) Every time step in the decoder only
```

11. Bahdanau and Luong Attention    
https://youtu.be/2eqIUDjefNg   

12. Multiplication Attention   
https://youtu.be/1-OwCgrx1eQ   

13. Additive Attention   
https://youtu.be/93VfVWZ-IvY   

14. Quiz: Addictive and Multiplication Attention    
QUESTION 1 OF 2   
Which of the following are valid scoring methods for attention? 【a, c and d】
```
a) Concat/additive
b) Traveling Salesman
c) Dot product
d) General
```
QUESTION 2 OF 2   
What's the intuition behind using dot product as a scoring method? 【b】  
```
a) The usefulness of the commutative property of multiplication
b) The dot product of two vectors in word-embedding space is a measure of similarity between them
```

15. Computer: Vision Applications   
https://youtu.be/bhWwc4BYTYc   

16. Other Attention Methonds   
https://youtu.be/VmsR9FVpQiM   

17. The Transformer and Self-Attention   
https://youtu.be/F-XN72bQiMQ    

18. Notebook: Attention Basics   
https://drive.google.com/open?id=11q5p3y06it4MuS1WJtR5qr9bPvEyvVB8   

19. [Solution]: Attention Basics
https://drive.google.com/open?id=11p3UvmaYueZrViwiXWb2uyYdNsX0f7lV   

20. Outro     
Great job completing the deep learning attention section!

You should have a greater idea about how information is represented in data and how you can programmatically represent the most important information in that data.

As you move on to the lesson about Image Captioning, please keep in mind the flexibility of an encoder and decoder model. The decoder portion is perhaps the most difficult as you have to decide how to embed image and word vectors into a shape that an LSTM can take as input and learn from. Good luck!